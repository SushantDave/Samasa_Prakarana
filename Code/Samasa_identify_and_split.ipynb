{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9b5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Embedding, Reshape, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4b894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(dtest, model, char2idx, maxlen):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    total_samasa = 0\n",
    "    correct_samasa = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for data in dtest:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "    X_test = [[char2idx[c] for c in w] for w in inputs]\n",
    "    X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\", value=char2idx['*'])\n",
    "    \n",
    "    Y_test = targets\n",
    "    Y_test = pad_sequences(maxlen=maxlen, sequences=Y_test, padding=\"post\", value=0.0)\n",
    "    Y_test = np.array(Y_test).reshape(-1, maxlen, 1)\n",
    "   \n",
    "    startlist = []\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        test = X_test[i].reshape((-1, maxlen))\n",
    "        res = model.predict(test, verbose=0)\n",
    "        res = res.reshape((maxlen))\n",
    "        dup = np.copy(res)\n",
    "        act = Y_test[i].reshape((maxlen))\n",
    "\n",
    "        wordlen = 0\n",
    "        for j in range(maxlen):\n",
    "            if X_test[i][j] == char2idx['*']:\n",
    "                break\n",
    "            else:\n",
    "                wordlen = wordlen + 1\n",
    "\n",
    "        res = res[0:wordlen]\n",
    "        act = act[0:wordlen]\n",
    "        origres = res\n",
    "        \n",
    "        for j in range(wordlen):\n",
    "            if(res[j] >= 0.5):\n",
    "                res[j] = 1\n",
    "            else:\n",
    "                res[j] = 0\n",
    "                \n",
    "        ires = res.astype(int)\n",
    "        iact = act.astype(int)\n",
    "        temp = np.multiply(ires, iact)\n",
    "        total_samasa = total_samasa + np.sum(iact)\n",
    "        correct_samasa = correct_samasa + np.sum(temp)\n",
    "\n",
    "        comparison = ires == iact\n",
    "        \n",
    "        if comparison.all():\n",
    "            passed = passed + 1\n",
    "        else:\n",
    "            failed = failed + 1\n",
    "\n",
    "    print(passed)\n",
    "    print(failed)\n",
    "    print(passed*100/(passed+failed))\n",
    "    print(correct_samasa)\n",
    "    print(total_samasa)\n",
    "    print(correct_samasa*100/total_samasa)\n",
    "\n",
    "    return startlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7640fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17304/17304 [26:59<00:00, 10.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15036\n",
      "2268\n",
      "86.89320388349515\n",
      "23717\n",
      "25861\n",
      "91.70952399365841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and test files\n",
    "model = keras.models.load_model('stage1_bilstm.h5', compile=False)\n",
    "fh = open('stage1_char2idx.txt', 'r')\n",
    "data = fh.read()\n",
    "char2idx = eval(data)\n",
    "fh.close()\n",
    "file = open(\"dtest.csv\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "dtest = []\n",
    "for line in lines:\n",
    "    dtest.append(line.strip().split(','))\n",
    "test_model(dtest, model, char2idx, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f03cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:04<01:33,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pag pa-g ga-p\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:08<00:56,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aj A-Aj A-j\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [00:08<00:55,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akAD aka-aD aka-A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:13<00:47,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada A-Ada A-da\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:18<00:52,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atAr ata-ar ata-Ar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [00:19<00:50,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uv U-v a-v\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:20<00:56,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahoda ahA-uda aha-uda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:22<00:42,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atv a-tv at-v\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:27<00:33,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAra DA-ra Da-arT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:29<00:32,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radaz rat-az ra-dz\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:34<00:30,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agv ak-v a-v\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:42<00:24,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poz pa-uz pa-ud\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 66/100 [00:43<00:22,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm f-m i-m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:45<00:19,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arAN ara-aN ara-Ag\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:48<00:19,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anoma anaH-ma ana-uma\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 74/100 [00:49<00:21,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehAr eha-ar Aha-ar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [00:58<00:06,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zob zaH-b za-uk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:04<00:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jova jaH-va ja-uja\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 82/100, 82.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_encoder_seq_length = 6\n",
    "max_decoder_seq_length = 11\n",
    "num_tokens = 52\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, token_index['&']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '$' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def infer_sandhi_split(dtest, encoder_model, decoder_model, token_index):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for data in dtest:\n",
    "        [input_text, target_text] = data.split(',')\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_tokens), dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, char in enumerate(input_text):\n",
    "            if char not in token_index:\n",
    "                continue\n",
    "            encoder_input_data[i, t, token_index[char]] = 1.\n",
    "        encoder_input_data[i, t + 1:, token_index['*']] = 1.\n",
    "    \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_char_index = dict((i, char) for char, i in token_index.items())\n",
    "    reverse_target_char_index = dict((i, char) for char, i in token_index.items())\n",
    "    \n",
    "    total = len(encoder_input_data)\n",
    "    passed = 0\n",
    "    results = []\n",
    "    for seq_index in tqdm(range(len(encoder_input_data))):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index)\n",
    "        decoded_sentence = decoded_sentence.strip()\n",
    "        decoded_sentence = decoded_sentence.strip('$')\n",
    "        results.append(decoded_sentence)\n",
    "        if decoded_sentence.strip() == target_texts[seq_index]:\n",
    "            passed = passed + 1\n",
    "        else:\n",
    "            print(input_texts[seq_index]+\" \"+str(target_texts[seq_index])+\" \"+str(decoded_sentence))\n",
    "\n",
    "    print(\"Passed: \"+str(passed)+'/'+str(total)+', '+str(passed*100/total))\n",
    "\n",
    "\n",
    "encoder_model = keras.models.load_model('stage2_encoder.h5')\n",
    "decoder_model = keras.models.load_model('stage2_decoder.h5')\n",
    "fh = open('stage2_token_index.txt', 'r')\n",
    "data = fh.read()\n",
    "token_index = eval(data)\n",
    "fh.close()\n",
    "file = open(\"stage2_dtest.csv\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "dtest = []\n",
    "for line in lines:\n",
    "    dtest.append(line.strip())\n",
    "\n",
    "infer_sandhi_split(dtest[0:100], encoder_model, decoder_model, token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19cd9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb682d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
