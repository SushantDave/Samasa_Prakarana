{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "997f12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\Downloads\\parallel_data_iast.tsv\", 'r', encoding='utf-8') as f:\n",
    "    itrans_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebcbd3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130270"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itrans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e127e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "error_cnt = 0\n",
    "slp1_list = []\n",
    "fp = open(\"D:\\Downloads\\parallel_data_slp1.tsv\", 'w', encoding='utf8')\n",
    "for line in itrans_list:\n",
    "    slp1_line = transliterate(line, sanscript.IAST, sanscript.SLP1)\n",
    "    slp1_list.append(slp1_line)\n",
    "    fp.write(slp1_line)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7eaabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp1_list = []\n",
    "fp = open(\"D:\\Downloads\\parallel_data_slp1.tsv\", 'r', encoding='utf8')\n",
    "slp1_lines = fp.readlines()\n",
    "fp.close()\n",
    "for slp1_line in slp1_lines:\n",
    "    slp1_list.append(slp1_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0ce3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_list = []\n",
    "for line in slp1_list:\n",
    "    elements = line.split(\"\\t\")\n",
    "    if(len(elements) != 3):\n",
    "        error_cnt += 1\n",
    "        print(line)\n",
    "        if(error_cnt > 5):\n",
    "            break\n",
    "    in_words = elements[1].split()\n",
    "    out_words = elements[2].split()\n",
    "    \n",
    "    #Remove duplicates in input and output sentences\n",
    "    in_words, out_words = list(set(in_words) - set(out_words)), list(set(out_words) - set(in_words))\n",
    "    \n",
    "    io_list.append([in_words, out_words])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdd3104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130270/130270 [59:06<00:00, 36.73it/s]  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_min_dist_window(word2, word1):\n",
    "    min_dist = 10000\n",
    "    min_start_idx = 0\n",
    "    min_word_len = 0\n",
    "    for start_id in range(len(word1)-1):\n",
    "        for window_len in range(1, len(word1)-start_id+1):\n",
    "            dist = nltk.edit_distance(word2, word1[start_id:start_id + window_len])\n",
    "            if(dist <= min_dist):\n",
    "                min_dist = dist\n",
    "                min_start_idx = start_id\n",
    "                min_word_len = window_len\n",
    "    return [min_dist, min_start_idx, min_word_len]\n",
    "\n",
    "def find_min_dist_idx(word2, sen1):\n",
    "    word2 = word2.replace('-', '')\n",
    "    min_dist = 10000\n",
    "    min_idx = 0\n",
    "    for idx in range(len(sen1)):\n",
    "        [dist, sid, wlen] = find_min_dist_window(word2, sen1[idx])\n",
    "        if(dist < min_dist):\n",
    "            min_dist = dist\n",
    "            min_word = sen1[idx][sid:sid + wlen]\n",
    "    return [min_dist, min_word]\n",
    "\n",
    "final_io_list = []\n",
    "for io in tqdm(io_list):\n",
    "    sen1 = io[0]\n",
    "    sen2 = io[1]\n",
    "    for cnt in range(len(sen2)):\n",
    "        if('-' in sen2[cnt]):\n",
    "            [min_dist, min_word] = find_min_dist_idx(sen2[cnt], sen1)\n",
    "            word1 = min_word\n",
    "            word2 = sen2[cnt]\n",
    "            word2 = word2.replace('-', '')\n",
    "            dist = min_dist/max(len(word1), len(word2))\n",
    "            final_io_list.append(word1+','+sen2[cnt]+','+('%.2f' % dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0b209b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127845\n",
      "86573\n"
     ]
    }
   ],
   "source": [
    "print(len(final_io_list))\n",
    "final_set = set(final_io_list)\n",
    "print(len(final_set))\n",
    "final_io_list = list(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "073c41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"sample_data_slp1.csv\", 'w', encoding='utf8')\n",
    "for io in final_io_list:\n",
    "    fp.write(io+'\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87466c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\Downloads\\sample_data_slp1.csv\", 'r', encoding='utf-8') as f:\n",
    "    data_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3add739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86532/86532 [00:46<00:00, 1877.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_mapping_len(word2, word1, start_id):\n",
    "    min_dist = 10000\n",
    "    min_word_len = 0\n",
    "    for curr_id in range(start_id+1, len(word1)-1):\n",
    "        dist = nltk.edit_distance(word2, word1[start_id:curr_id])\n",
    "        if(dist <= min_dist):\n",
    "            min_dist = dist\n",
    "            min_word_len = curr_id - start_id\n",
    "    #print(min_word_len)\n",
    "    return min_word_len\n",
    "\n",
    "seq_data_list = []\n",
    "\n",
    "# Presence of 1 marks a new word. Basically, 1 implies beginning of new word,\n",
    "# (except for 1st word), with zero or more characters following it (denoted by 0) \n",
    "for data in tqdm(data_list):\n",
    "    [word1, word2, dist] = data.split(',')\n",
    "    nword2 = word2.replace('-','')\n",
    "    if float(dist) == 0.00 or len(word1) == len(nword2):\n",
    "        wlist = word2.split('-')\n",
    "        #outword = '0'*(len(wlist[0])-1) + '1'\n",
    "        outword = '0'*(len(wlist[0]))\n",
    "        for widx in range(1, len(wlist)-1):\n",
    "            #outword = outword + '1' + '0'*(len(wlist[widx])-2) + '1'\n",
    "            outword = outword + '1' + '0'*(len(wlist[widx])-1)\n",
    "        outword = outword + '1' + '0'*(len(wlist[-1])-1)\n",
    "    else:\n",
    "        wlist = word2.split('-')\n",
    "        start_id = 0\n",
    "        wlen = find_mapping_len(wlist[0], word1, start_id)\n",
    "        #outword = '0'*(wlen-1) + '1'\n",
    "        outword = '0'*(wlen)\n",
    "        start_id = start_id + wlen\n",
    "        for widx in range(1, len(wlist)-1):\n",
    "            wlen = find_mapping_len(wlist[widx], word1, start_id)\n",
    "            #outword = outword + '1' + '0'*(wlen-2) + '1'\n",
    "            outword = outword + '1' + '0'*(wlen-1)\n",
    "            start_id = start_id + wlen\n",
    "        outword = outword + '1' + '0'*(len(word1)-start_id-1)\n",
    "\n",
    "    #print(word1)\n",
    "    #print(word2)\n",
    "    #print(outword)\n",
    "    seq_data_list.append([word1, word2, outword, dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73ba7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"final_data_slp1.csv\", 'w', encoding='utf8')\n",
    "for io in seq_data_list:\n",
    "    if(len(io[0]) != len(io[2])):\n",
    "        print(io[0])\n",
    "        break\n",
    "    if(io[1].count('-') != io[2].count('1')):\n",
    "        print(io[0])\n",
    "        break\n",
    "    fp.write(io[0]+','+io[1]+','+io[2]+','+io[3])\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e72d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "paNktiviMSatitriMSaccatvAriMSatpaYcASatzazwisaptatyaSItinavatiSatam\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "with open(\"final_data_slp1.csv\", 'r', encoding='utf8') as fp:\n",
    "    iol = fp.readlines()\n",
    "    \n",
    "for io in iol:\n",
    "    wd = io.split(',')[0]\n",
    "    if(len(wd) > maxlen):\n",
    "        maxlen = len(wd)\n",
    "        maxword = wd\n",
    "print(maxlen)\n",
    "print(maxword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe5d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
