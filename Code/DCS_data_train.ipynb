{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe7df58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Embedding, Reshape, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b442f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozaDigaRAH\n",
      "72\n",
      "Number of training samples: 69224\n",
      "Number of unique tokens: 49\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 72)]              0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 72, 8)             392       \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  [(None, 72, 128),         37376     \n",
      " ional)                       (None, 64),                        \n",
      "                              (None, 64),                        \n",
      "                              (None, 64),                        \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 72, 128)           0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 72, 1)             129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37897 (148.04 KB)\n",
      "Trainable params: 37897 (148.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "974/974 [==============================] - 124s 117ms/step - loss: 65.6548 - accuracy: 0.9847 - val_loss: 42.0820 - val_accuracy: 0.9896\n",
      "Epoch 2/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 41.7305 - accuracy: 0.9896 - val_loss: 38.0332 - val_accuracy: 0.9905\n",
      "Epoch 3/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 38.2482 - accuracy: 0.9905 - val_loss: 35.7671 - val_accuracy: 0.9911\n",
      "Epoch 4/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 36.2679 - accuracy: 0.9911 - val_loss: 33.8898 - val_accuracy: 0.9917\n",
      "Epoch 5/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 34.4427 - accuracy: 0.9916 - val_loss: 32.0497 - val_accuracy: 0.9923\n",
      "Epoch 6/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 32.6113 - accuracy: 0.9922 - val_loss: 30.4354 - val_accuracy: 0.9928\n",
      "Epoch 7/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 30.8912 - accuracy: 0.9927 - val_loss: 28.9154 - val_accuracy: 0.9932\n",
      "Epoch 8/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 29.6087 - accuracy: 0.9931 - val_loss: 27.4768 - val_accuracy: 0.9937\n",
      "Epoch 9/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 28.3952 - accuracy: 0.9935 - val_loss: 26.3086 - val_accuracy: 0.9941\n",
      "Epoch 10/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 27.3618 - accuracy: 0.9938 - val_loss: 25.2773 - val_accuracy: 0.9943\n",
      "Epoch 11/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 26.3951 - accuracy: 0.9941 - val_loss: 24.4260 - val_accuracy: 0.9946\n",
      "Epoch 12/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 25.5388 - accuracy: 0.9943 - val_loss: 23.4979 - val_accuracy: 0.9949\n",
      "Epoch 13/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 24.6768 - accuracy: 0.9946 - val_loss: 23.0972 - val_accuracy: 0.9950\n",
      "Epoch 14/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 23.9062 - accuracy: 0.9949 - val_loss: 22.1540 - val_accuracy: 0.9953\n",
      "Epoch 15/60\n",
      "974/974 [==============================] - 122s 126ms/step - loss: 23.1481 - accuracy: 0.9951 - val_loss: 21.6634 - val_accuracy: 0.9955\n",
      "Epoch 16/60\n",
      "974/974 [==============================] - 120s 123ms/step - loss: 22.5160 - accuracy: 0.9953 - val_loss: 21.0856 - val_accuracy: 0.9956\n",
      "Epoch 17/60\n",
      "974/974 [==============================] - 121s 124ms/step - loss: 21.8469 - accuracy: 0.9955 - val_loss: 20.6960 - val_accuracy: 0.9958\n",
      "Epoch 18/60\n",
      "974/974 [==============================] - 115s 118ms/step - loss: 21.2954 - accuracy: 0.9956 - val_loss: 20.3688 - val_accuracy: 0.9958\n",
      "Epoch 19/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 20.8385 - accuracy: 0.9958 - val_loss: 19.9147 - val_accuracy: 0.9960\n",
      "Epoch 20/60\n",
      "974/974 [==============================] - 122s 126ms/step - loss: 20.3195 - accuracy: 0.9959 - val_loss: 19.6943 - val_accuracy: 0.9960\n",
      "Epoch 21/60\n",
      "974/974 [==============================] - 117s 120ms/step - loss: 19.9005 - accuracy: 0.9960 - val_loss: 19.2050 - val_accuracy: 0.9962\n",
      "Epoch 22/60\n",
      "974/974 [==============================] - 123s 127ms/step - loss: 19.5451 - accuracy: 0.9961 - val_loss: 18.8604 - val_accuracy: 0.9962\n",
      "Epoch 23/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 19.1292 - accuracy: 0.9962 - val_loss: 18.5834 - val_accuracy: 0.9964\n",
      "Epoch 24/60\n",
      "974/974 [==============================] - 110s 113ms/step - loss: 18.7150 - accuracy: 0.9964 - val_loss: 18.3145 - val_accuracy: 0.9965\n",
      "Epoch 25/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 18.3818 - accuracy: 0.9965 - val_loss: 18.2362 - val_accuracy: 0.9965\n",
      "Epoch 26/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 18.0174 - accuracy: 0.9966 - val_loss: 18.1239 - val_accuracy: 0.9965\n",
      "Epoch 27/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 17.7087 - accuracy: 0.9967 - val_loss: 17.7213 - val_accuracy: 0.9966\n",
      "Epoch 28/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 17.4639 - accuracy: 0.9967 - val_loss: 17.2392 - val_accuracy: 0.9967\n",
      "Epoch 29/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 17.0891 - accuracy: 0.9968 - val_loss: 17.3459 - val_accuracy: 0.9967\n",
      "Epoch 30/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 16.8594 - accuracy: 0.9969 - val_loss: 17.1613 - val_accuracy: 0.9967\n",
      "Epoch 31/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 16.6124 - accuracy: 0.9970 - val_loss: 17.0427 - val_accuracy: 0.9968\n",
      "Epoch 32/60\n",
      "974/974 [==============================] - 117s 120ms/step - loss: 16.3509 - accuracy: 0.9970 - val_loss: 16.7523 - val_accuracy: 0.9968\n",
      "Epoch 33/60\n",
      "974/974 [==============================] - 116s 119ms/step - loss: 16.1412 - accuracy: 0.9971 - val_loss: 16.6206 - val_accuracy: 0.9969\n",
      "Epoch 34/60\n",
      "974/974 [==============================] - 123s 126ms/step - loss: 15.9412 - accuracy: 0.9971 - val_loss: 16.3980 - val_accuracy: 0.9969\n",
      "Epoch 35/60\n",
      "974/974 [==============================] - 125s 128ms/step - loss: 15.6925 - accuracy: 0.9972 - val_loss: 16.3760 - val_accuracy: 0.9969\n",
      "Epoch 36/60\n",
      "974/974 [==============================] - 115s 118ms/step - loss: 15.4553 - accuracy: 0.9973 - val_loss: 16.2746 - val_accuracy: 0.9970\n",
      "Epoch 37/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 15.3062 - accuracy: 0.9973 - val_loss: 15.9980 - val_accuracy: 0.9971\n",
      "Epoch 38/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 15.0500 - accuracy: 0.9974 - val_loss: 16.1110 - val_accuracy: 0.9971\n",
      "Epoch 39/60\n",
      "974/974 [==============================] - 113s 116ms/step - loss: 14.9644 - accuracy: 0.9974 - val_loss: 15.8816 - val_accuracy: 0.9971\n",
      "Epoch 40/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.7623 - accuracy: 0.9975 - val_loss: 15.7368 - val_accuracy: 0.9971\n",
      "Epoch 41/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.6178 - accuracy: 0.9975 - val_loss: 15.6680 - val_accuracy: 0.9972\n",
      "Epoch 42/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.4571 - accuracy: 0.9976 - val_loss: 15.7432 - val_accuracy: 0.9971\n",
      "Epoch 43/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.3793 - accuracy: 0.9976 - val_loss: 15.6832 - val_accuracy: 0.9971\n",
      "Epoch 44/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.2264 - accuracy: 0.9976 - val_loss: 15.5073 - val_accuracy: 0.9972\n",
      "Epoch 45/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 14.0805 - accuracy: 0.9977 - val_loss: 15.6563 - val_accuracy: 0.9972\n",
      "Epoch 46/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974/974 [==============================] - 108s 111ms/step - loss: 13.9701 - accuracy: 0.9977 - val_loss: 15.2902 - val_accuracy: 0.9973\n",
      "Epoch 47/60\n",
      "974/974 [==============================] - 111s 114ms/step - loss: 13.8498 - accuracy: 0.9977 - val_loss: 15.2376 - val_accuracy: 0.9973\n",
      "Epoch 48/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.8062 - accuracy: 0.9977 - val_loss: 15.3760 - val_accuracy: 0.9972\n",
      "Epoch 49/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.6986 - accuracy: 0.9978 - val_loss: 15.3267 - val_accuracy: 0.9973\n",
      "Epoch 50/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 13.4733 - accuracy: 0.9979 - val_loss: 15.1477 - val_accuracy: 0.9973\n",
      "Epoch 51/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.4572 - accuracy: 0.9978 - val_loss: 15.3933 - val_accuracy: 0.9973\n",
      "Epoch 52/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.3343 - accuracy: 0.9979 - val_loss: 15.1976 - val_accuracy: 0.9973\n",
      "Epoch 53/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.2655 - accuracy: 0.9979 - val_loss: 14.9833 - val_accuracy: 0.9974\n",
      "Epoch 54/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 13.1630 - accuracy: 0.9979 - val_loss: 14.9080 - val_accuracy: 0.9974\n",
      "Epoch 55/60\n",
      "974/974 [==============================] - 109s 111ms/step - loss: 13.1047 - accuracy: 0.9979 - val_loss: 14.9628 - val_accuracy: 0.9974\n",
      "Epoch 56/60\n",
      "974/974 [==============================] - 109s 111ms/step - loss: 12.9717 - accuracy: 0.9980 - val_loss: 14.7197 - val_accuracy: 0.9975\n",
      "Epoch 57/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 12.9544 - accuracy: 0.9980 - val_loss: 15.1914 - val_accuracy: 0.9973\n",
      "Epoch 58/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 12.8504 - accuracy: 0.9980 - val_loss: 15.1561 - val_accuracy: 0.9973\n",
      "Epoch 59/60\n",
      "974/974 [==============================] - 108s 111ms/step - loss: 12.7562 - accuracy: 0.9980 - val_loss: 14.8611 - val_accuracy: 0.9974\n",
      "Epoch 60/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 12.7309 - accuracy: 0.9980 - val_loss: 14.8851 - val_accuracy: 0.9974\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    loss1 = mse(y_true, y_pred)\n",
    "    pred_mean = tf.reduce_mean(y_pred)\n",
    "    true_mean = tf.cast(tf.reduce_mean(y_true), tf.float32)\n",
    "    loss2 = (pred_mean - true_mean)**2\n",
    "    loss3 = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_true, tf.float32), y_pred)))\n",
    "    loss = loss3 #loss1 + 0.5*loss2\n",
    "    return loss\n",
    "\n",
    "def train_predict_sandhi_window(dtrain, dtest):\n",
    "    batch_size = 64  # Batch size for training.\n",
    "    epochs = 60  # Number of epochs to train for.\n",
    "    latent_dim = 64  # Latent dimensionality of the encoding space.\n",
    "\n",
    "    # Vectorize the data.\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    characters = set()\n",
    "    \n",
    "    for data in dtrain:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "        for char in input_word:\n",
    "            if char not in characters:\n",
    "                characters.add(char)\n",
    "\n",
    "    maxlen = max([len(s) for s in inputs])\n",
    "    print(inputs[0])\n",
    "    print(maxlen)\n",
    "\n",
    "    \"\"\"\n",
    "    * is used as padding character\n",
    "    \"\"\"\n",
    "    characters.add('*')\n",
    "    char2idx = dict([(char, i) for i, char in enumerate(characters)])\n",
    "    num_tokens = len(characters)\n",
    "    \n",
    "    X_train = [[char2idx[c] for c in w] for w in inputs]\n",
    "    X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\", value=char2idx['*'])\n",
    "    \n",
    "    Y_train = targets\n",
    "    Y_train = pad_sequences(maxlen=maxlen, sequences=Y_train, padding=\"post\", value=0.0)\n",
    "    Y_train = np.array(Y_train).reshape(-1, maxlen, 1)\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for data in dtest:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "        for char in input_word:\n",
    "            if char not in characters:\n",
    "                characters.add(char)\n",
    "    \n",
    "    print('Number of training samples:', len(X_train))\n",
    "    print('Number of unique tokens:', num_tokens)\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    inputword = Input(shape=(maxlen,))\n",
    "    embed = Embedding(input_dim=num_tokens, output_dim=8, input_length=maxlen, mask_zero=True)(inputword)\n",
    "    bilstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    out, forward_h, forward_c, backward_h, backward_c = bilstm(embed)\n",
    "    outd = Dropout(0.5)(out)\n",
    "    outputtarget = Dense(1, activation=\"sigmoid\")(outd)\n",
    "    \n",
    "    model = Model(inputword, outputtarget)\n",
    "    #model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    model.compile(optimizer='rmsprop', loss=custom_loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(X_train, Y_train, batch_size, epochs, validation_split=0.1)\n",
    "    return model, char2idx, maxlen\n",
    "    \n",
    "    # Save model and test files\n",
    "    #model.save('bilstm.h5')\n",
    "    #np.save('testX', X_test)\n",
    "    #np.save('testY', Y_test)\n",
    "\n",
    "with open(\"final_data_slp1.csv\", 'r', encoding='utf-8') as f:\n",
    "    odl = f.readlines()\n",
    "dl = []\n",
    "for ol in odl:\n",
    "    lol = ol.split(',')\n",
    "    dl.append([lol[0], lol[2]])\n",
    "\n",
    "dtrain, dtest = train_test_split(dl, test_size=0.2, random_state=1)\n",
    "model, char2idx, maxlen = train_predict_sandhi_window(dtrain, dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f069db08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17307/17307 [25:53<00:00, 11.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13712\n",
      "3595\n",
      "79.22805801120934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(dtest, model, char2idx, maxlen):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for data in dtest:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "    X_test = [[char2idx[c] for c in w] for w in inputs]\n",
    "    X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\", value=char2idx['*'])\n",
    "    \n",
    "    Y_test = targets\n",
    "    Y_test = pad_sequences(maxlen=maxlen, sequences=Y_test, padding=\"post\", value=0.0)\n",
    "    Y_test = np.array(Y_test).reshape(-1, maxlen, 1)\n",
    "   \n",
    "    startlist = []\n",
    "    fp = open(\"failed.txt\", 'w')\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        test = X_test[i].reshape((-1, maxlen))\n",
    "        #print(test)\n",
    "        #print(test.shape)\n",
    "        res = model.predict(test, verbose=0)\n",
    "        res = res.reshape((maxlen))\n",
    "        #print(res)\n",
    "        #print(res.shape)\n",
    "        dup = np.copy(res)\n",
    "        act = Y_test[i].reshape((maxlen))\n",
    "        #print(act)\n",
    "        #print(act.shape)\n",
    "\n",
    "        wordlen = 0\n",
    "        for j in range(maxlen):\n",
    "            if X_test[i][j] == char2idx['*']:\n",
    "                break\n",
    "            else:\n",
    "                wordlen = wordlen + 1\n",
    "\n",
    "        res = res[0:wordlen]\n",
    "        act = act[0:wordlen]\n",
    "        origres = res\n",
    "        \n",
    "        for j in range(wordlen):\n",
    "            if(res[j] >= 0.5):\n",
    "                res[j] = 1\n",
    "            else:\n",
    "                res[j] = 0\n",
    "                \n",
    "        ires = res.astype(int)\n",
    "        iact = act.astype(int)\n",
    "\n",
    "        comparison = ires == iact\n",
    "        \n",
    "        if comparison.all():\n",
    "            passed = passed + 1\n",
    "        else:\n",
    "            failed = failed + 1\n",
    "            fp.write(str(origres))\n",
    "            fp.write(str(ires))\n",
    "            fp.write(str(iact))\n",
    "            fp.write('\\n')\n",
    "            \"\"\"\n",
    "                print(act)\n",
    "                print(dup)\n",
    "                print(\"****************************************************\")\n",
    "            \"\"\"\n",
    "    fp.close()\n",
    "    print(passed)\n",
    "    print(failed)\n",
    "    print(passed*100/(passed+failed))\n",
    "\n",
    "    return startlist\n",
    "\n",
    "test_model(dtest, model, char2idx, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb549473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save model and test files\n",
    "model.save('bilstm.h5')\n",
    "fh = open('char2idx.txt', 'w')\n",
    "data = str(char2idx)\n",
    "fh.write(data)\n",
    "fh.close()\n",
    "import csv\n",
    "with open(\"dtest.csv\", \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ee2de3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86533fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
