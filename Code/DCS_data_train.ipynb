{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe7df58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Embedding, Reshape, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b442f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viparyayopArohI\n",
      "72\n",
      "Number of training samples: 69214\n",
      "Number of unique tokens: 49\n",
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 72)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 72, 8)             392       \n",
      "                                                                 \n",
      " bidirectional (Bidirection  [(None, 72, 128),         37376     \n",
      " al)                          (None, 64),                        \n",
      "                              (None, 64),                        \n",
      "                              (None, 64),                        \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 72, 128)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 72, 1)             129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37897 (148.04 KB)\n",
      "Trainable params: 37897 (148.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sushant\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "974/974 [==============================] - 125s 114ms/step - loss: 62.0648 - accuracy: 0.9840 - val_loss: 38.0722 - val_accuracy: 0.9895\n",
      "Epoch 2/60\n",
      "974/974 [==============================] - 129s 132ms/step - loss: 38.2821 - accuracy: 0.9894 - val_loss: 33.7363 - val_accuracy: 0.9907\n",
      "Epoch 3/60\n",
      "974/974 [==============================] - 123s 127ms/step - loss: 33.6472 - accuracy: 0.9907 - val_loss: 30.9285 - val_accuracy: 0.9914\n",
      "Epoch 4/60\n",
      "974/974 [==============================] - 119s 122ms/step - loss: 30.3847 - accuracy: 0.9916 - val_loss: 27.6357 - val_accuracy: 0.9924\n",
      "Epoch 5/60\n",
      "974/974 [==============================] - 122s 125ms/step - loss: 28.1687 - accuracy: 0.9922 - val_loss: 26.3256 - val_accuracy: 0.9927\n",
      "Epoch 6/60\n",
      "974/974 [==============================] - 139s 143ms/step - loss: 26.4255 - accuracy: 0.9927 - val_loss: 24.5027 - val_accuracy: 0.9933\n",
      "Epoch 7/60\n",
      "974/974 [==============================] - 109s 111ms/step - loss: 24.9154 - accuracy: 0.9932 - val_loss: 22.9862 - val_accuracy: 0.9938\n",
      "Epoch 8/60\n",
      "974/974 [==============================] - 114s 117ms/step - loss: 23.6291 - accuracy: 0.9936 - val_loss: 21.8142 - val_accuracy: 0.9941\n",
      "Epoch 9/60\n",
      "974/974 [==============================] - 123s 126ms/step - loss: 22.6637 - accuracy: 0.9939 - val_loss: 21.0142 - val_accuracy: 0.9943\n",
      "Epoch 10/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 21.6800 - accuracy: 0.9941 - val_loss: 20.0192 - val_accuracy: 0.9945\n",
      "Epoch 11/60\n",
      "974/974 [==============================] - 114s 117ms/step - loss: 20.8015 - accuracy: 0.9944 - val_loss: 19.5556 - val_accuracy: 0.9947\n",
      "Epoch 12/60\n",
      "974/974 [==============================] - 96s 99ms/step - loss: 19.9899 - accuracy: 0.9946 - val_loss: 18.5411 - val_accuracy: 0.9950\n",
      "Epoch 13/60\n",
      "974/974 [==============================] - 95s 98ms/step - loss: 19.1887 - accuracy: 0.9949 - val_loss: 18.0368 - val_accuracy: 0.9952\n",
      "Epoch 14/60\n",
      "974/974 [==============================] - 101s 104ms/step - loss: 18.4843 - accuracy: 0.9951 - val_loss: 17.2426 - val_accuracy: 0.9954\n",
      "Epoch 15/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 17.6871 - accuracy: 0.9953 - val_loss: 16.6774 - val_accuracy: 0.9955\n",
      "Epoch 16/60\n",
      "974/974 [==============================] - 109s 112ms/step - loss: 16.9318 - accuracy: 0.9955 - val_loss: 16.0404 - val_accuracy: 0.9957\n",
      "Epoch 17/60\n",
      "974/974 [==============================] - 113s 115ms/step - loss: 16.3028 - accuracy: 0.9957 - val_loss: 15.5182 - val_accuracy: 0.9959\n",
      "Epoch 18/60\n",
      "974/974 [==============================] - 130s 133ms/step - loss: 15.6761 - accuracy: 0.9959 - val_loss: 15.1166 - val_accuracy: 0.9959\n",
      "Epoch 19/60\n",
      "974/974 [==============================] - 124s 127ms/step - loss: 15.1484 - accuracy: 0.9960 - val_loss: 14.6430 - val_accuracy: 0.9960\n",
      "Epoch 20/60\n",
      "974/974 [==============================] - 124s 128ms/step - loss: 14.6964 - accuracy: 0.9961 - val_loss: 14.0514 - val_accuracy: 0.9963\n",
      "Epoch 21/60\n",
      "974/974 [==============================] - 130s 133ms/step - loss: 14.2948 - accuracy: 0.9962 - val_loss: 13.9873 - val_accuracy: 0.9962\n",
      "Epoch 22/60\n",
      "974/974 [==============================] - 121s 124ms/step - loss: 13.8100 - accuracy: 0.9964 - val_loss: 13.4228 - val_accuracy: 0.9964\n",
      "Epoch 23/60\n",
      "974/974 [==============================] - 120s 123ms/step - loss: 13.4304 - accuracy: 0.9965 - val_loss: 13.2824 - val_accuracy: 0.9964\n",
      "Epoch 24/60\n",
      "974/974 [==============================] - 113s 116ms/step - loss: 13.0190 - accuracy: 0.9966 - val_loss: 12.6993 - val_accuracy: 0.9966\n",
      "Epoch 25/60\n",
      "974/974 [==============================] - 123s 126ms/step - loss: 12.6695 - accuracy: 0.9967 - val_loss: 12.4285 - val_accuracy: 0.9967\n",
      "Epoch 26/60\n",
      "974/974 [==============================] - 112s 115ms/step - loss: 12.4186 - accuracy: 0.9968 - val_loss: 12.5820 - val_accuracy: 0.9967\n",
      "Epoch 27/60\n",
      "974/974 [==============================] - 117s 120ms/step - loss: 12.1526 - accuracy: 0.9968 - val_loss: 12.2205 - val_accuracy: 0.9967\n",
      "Epoch 28/60\n",
      "974/974 [==============================] - 115s 118ms/step - loss: 11.8595 - accuracy: 0.9969 - val_loss: 12.0671 - val_accuracy: 0.9968\n",
      "Epoch 29/60\n",
      "974/974 [==============================] - 102s 105ms/step - loss: 11.5186 - accuracy: 0.9970 - val_loss: 11.7837 - val_accuracy: 0.9969\n",
      "Epoch 30/60\n",
      "974/974 [==============================] - 98s 100ms/step - loss: 11.3661 - accuracy: 0.9971 - val_loss: 11.6909 - val_accuracy: 0.9969\n",
      "Epoch 31/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 11.1062 - accuracy: 0.9972 - val_loss: 11.8126 - val_accuracy: 0.9969\n",
      "Epoch 32/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 10.9389 - accuracy: 0.9972 - val_loss: 11.4774 - val_accuracy: 0.9970\n",
      "Epoch 33/60\n",
      "974/974 [==============================] - 98s 100ms/step - loss: 10.7308 - accuracy: 0.9972 - val_loss: 11.0869 - val_accuracy: 0.9971\n",
      "Epoch 34/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 10.5108 - accuracy: 0.9973 - val_loss: 11.0265 - val_accuracy: 0.9972\n",
      "Epoch 35/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 10.3203 - accuracy: 0.9974 - val_loss: 10.8724 - val_accuracy: 0.9972\n",
      "Epoch 36/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 10.1063 - accuracy: 0.9974 - val_loss: 10.8744 - val_accuracy: 0.9972\n",
      "Epoch 37/60\n",
      "974/974 [==============================] - 97s 100ms/step - loss: 9.9699 - accuracy: 0.9975 - val_loss: 10.5377 - val_accuracy: 0.9973\n",
      "Epoch 38/60\n",
      "974/974 [==============================] - 99s 101ms/step - loss: 9.7474 - accuracy: 0.9975 - val_loss: 10.6714 - val_accuracy: 0.9972\n",
      "Epoch 39/60\n",
      "974/974 [==============================] - 98s 101ms/step - loss: 9.6623 - accuracy: 0.9975 - val_loss: 10.4173 - val_accuracy: 0.9973\n",
      "Epoch 40/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974/974 [==============================] - 94s 96ms/step - loss: 9.4792 - accuracy: 0.9976 - val_loss: 10.0646 - val_accuracy: 0.9974\n",
      "Epoch 41/60\n",
      "974/974 [==============================] - 93s 96ms/step - loss: 9.2602 - accuracy: 0.9976 - val_loss: 10.1750 - val_accuracy: 0.9974\n",
      "Epoch 42/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 9.1159 - accuracy: 0.9977 - val_loss: 10.1226 - val_accuracy: 0.9974\n",
      "Epoch 43/60\n",
      "974/974 [==============================] - 95s 97ms/step - loss: 9.0136 - accuracy: 0.9977 - val_loss: 10.1732 - val_accuracy: 0.9974\n",
      "Epoch 44/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 8.8294 - accuracy: 0.9978 - val_loss: 9.8307 - val_accuracy: 0.9975\n",
      "Epoch 45/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 8.7073 - accuracy: 0.9978 - val_loss: 9.8680 - val_accuracy: 0.9975\n",
      "Epoch 46/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 8.5926 - accuracy: 0.9978 - val_loss: 9.6629 - val_accuracy: 0.9975\n",
      "Epoch 47/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 8.5034 - accuracy: 0.9978 - val_loss: 9.8085 - val_accuracy: 0.9975\n",
      "Epoch 48/60\n",
      "974/974 [==============================] - 95s 97ms/step - loss: 8.2963 - accuracy: 0.9979 - val_loss: 9.8188 - val_accuracy: 0.9975\n",
      "Epoch 49/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 8.2996 - accuracy: 0.9979 - val_loss: 9.4913 - val_accuracy: 0.9976\n",
      "Epoch 50/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 8.1715 - accuracy: 0.9979 - val_loss: 9.4860 - val_accuracy: 0.9976\n",
      "Epoch 51/60\n",
      "974/974 [==============================] - 95s 97ms/step - loss: 8.0384 - accuracy: 0.9980 - val_loss: 9.5179 - val_accuracy: 0.9976\n",
      "Epoch 52/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 8.0156 - accuracy: 0.9980 - val_loss: 9.4743 - val_accuracy: 0.9976\n",
      "Epoch 53/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 7.9001 - accuracy: 0.9980 - val_loss: 9.3599 - val_accuracy: 0.9976\n",
      "Epoch 54/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 7.8215 - accuracy: 0.9980 - val_loss: 9.3340 - val_accuracy: 0.9976\n",
      "Epoch 55/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 7.7762 - accuracy: 0.9980 - val_loss: 9.1769 - val_accuracy: 0.9977\n",
      "Epoch 56/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 7.6794 - accuracy: 0.9981 - val_loss: 9.1072 - val_accuracy: 0.9977\n",
      "Epoch 57/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 7.5518 - accuracy: 0.9981 - val_loss: 9.0930 - val_accuracy: 0.9977\n",
      "Epoch 58/60\n",
      "974/974 [==============================] - 94s 97ms/step - loss: 7.5266 - accuracy: 0.9981 - val_loss: 9.2130 - val_accuracy: 0.9976\n",
      "Epoch 59/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 7.4002 - accuracy: 0.9981 - val_loss: 9.1235 - val_accuracy: 0.9977\n",
      "Epoch 60/60\n",
      "974/974 [==============================] - 94s 96ms/step - loss: 7.3141 - accuracy: 0.9982 - val_loss: 9.2832 - val_accuracy: 0.9976\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    loss1 = mse(y_true, y_pred)\n",
    "    pred_mean = tf.reduce_mean(y_pred)\n",
    "    true_mean = tf.cast(tf.reduce_mean(y_true), tf.float32)\n",
    "    loss2 = (pred_mean - true_mean)**2\n",
    "    loss3 = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_true, tf.float32), y_pred)))\n",
    "    loss = loss3 #loss1 + 0.5*loss2\n",
    "    return loss\n",
    "\n",
    "def train_predict_sandhi_window(dtrain, dtest):\n",
    "    batch_size = 64  # Batch size for training.\n",
    "    epochs = 60  # Number of epochs to train for.\n",
    "    latent_dim = 64  # Latent dimensionality of the encoding space.\n",
    "\n",
    "    # Vectorize the data.\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    characters = set()\n",
    "    \n",
    "    for data in dtrain:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "        for char in input_word:\n",
    "            if char not in characters:\n",
    "                characters.add(char)\n",
    "\n",
    "    maxlen = max([len(s) for s in inputs])\n",
    "    print(inputs[0])\n",
    "    print(maxlen)\n",
    "\n",
    "    \"\"\"\n",
    "    * is used as padding character\n",
    "    \"\"\"\n",
    "    characters.add('*')\n",
    "    char2idx = dict([(char, i) for i, char in enumerate(characters)])\n",
    "    num_tokens = len(characters)\n",
    "    \n",
    "    X_train = [[char2idx[c] for c in w] for w in inputs]\n",
    "    X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\", value=char2idx['*'])\n",
    "    \n",
    "    Y_train = targets\n",
    "    Y_train = pad_sequences(maxlen=maxlen, sequences=Y_train, padding=\"post\", value=0.0)\n",
    "    Y_train = np.array(Y_train).reshape(-1, maxlen, 1)\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for data in dtest:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "        for char in input_word:\n",
    "            if char not in characters:\n",
    "                characters.add(char)\n",
    "    \n",
    "    print('Number of training samples:', len(X_train))\n",
    "    print('Number of unique tokens:', num_tokens)\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    inputword = Input(shape=(maxlen,))\n",
    "    embed = Embedding(input_dim=num_tokens, output_dim=8, input_length=maxlen, mask_zero=True)(inputword)\n",
    "    bilstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    out, forward_h, forward_c, backward_h, backward_c = bilstm(embed)\n",
    "    outd = Dropout(0.5)(out)\n",
    "    outputtarget = Dense(1, activation=\"sigmoid\")(outd)\n",
    "    \n",
    "    model = Model(inputword, outputtarget)\n",
    "    model.compile(optimizer='rmsprop', loss=custom_loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(X_train, Y_train, batch_size, epochs, validation_split=0.1)\n",
    "    return model, char2idx, maxlen\n",
    "    \n",
    "with open(\"final_data_slp1.csv\", 'r', encoding='utf-8') as f:\n",
    "    odl = f.readlines()\n",
    "dl = []\n",
    "for ol in odl:\n",
    "    lol = ol.split(',')\n",
    "    dl.append([lol[0], lol[2]])\n",
    "\n",
    "dtrain, dtest = train_test_split(dl, test_size=0.2, random_state=1)\n",
    "model, char2idx, maxlen = train_predict_sandhi_window(dtrain, dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f069db08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17304/17304 [26:22<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15036\n",
      "2268\n",
      "86.89320388349515\n",
      "23717\n",
      "25861\n",
      "91.70952399365841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(dtest, model, char2idx, maxlen):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    total_samasa = 0\n",
    "    correct_samasa = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for data in dtest:\n",
    "        target = np.array(list(data[1]))\n",
    "        input_word = data[0]\n",
    "    \n",
    "        inputs.append(input_word)\n",
    "        targets.append(target)\n",
    "    \n",
    "    X_test = [[char2idx[c] for c in w] for w in inputs]\n",
    "    X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\", value=char2idx['*'])\n",
    "    \n",
    "    Y_test = targets\n",
    "    Y_test = pad_sequences(maxlen=maxlen, sequences=Y_test, padding=\"post\", value=0.0)\n",
    "    Y_test = np.array(Y_test).reshape(-1, maxlen, 1)\n",
    "   \n",
    "    startlist = []\n",
    "    fp = open(\"failed.txt\", 'w')\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        test = X_test[i].reshape((-1, maxlen))\n",
    "        res = model.predict(test, verbose=0)\n",
    "        res = res.reshape((maxlen))\n",
    "        dup = np.copy(res)\n",
    "        act = Y_test[i].reshape((maxlen))\n",
    "\n",
    "        wordlen = 0\n",
    "        for j in range(maxlen):\n",
    "            if X_test[i][j] == char2idx['*']:\n",
    "                break\n",
    "            else:\n",
    "                wordlen = wordlen + 1\n",
    "\n",
    "        res = res[0:wordlen]\n",
    "        act = act[0:wordlen]\n",
    "        origres = res\n",
    "        \n",
    "        for j in range(wordlen):\n",
    "            if(res[j] >= 0.5):\n",
    "                res[j] = 1\n",
    "            else:\n",
    "                res[j] = 0\n",
    "                \n",
    "        ires = res.astype(int)\n",
    "        iact = act.astype(int)\n",
    "        temp = np.multiply(ires, iact)\n",
    "        total_samasa = total_samasa + np.sum(iact)\n",
    "        correct_samasa = correct_samasa + np.sum(temp)\n",
    "\n",
    "        comparison = ires == iact\n",
    "        \n",
    "        if comparison.all():\n",
    "            passed = passed + 1\n",
    "        else:\n",
    "            failed = failed + 1\n",
    "            fp.write(str(ires))\n",
    "            fp.write('\\n')\n",
    "            fp.write(str(iact))\n",
    "            fp.write('\\n')\n",
    "            fp.write('*****************************************************\\n')\n",
    "\n",
    "    fp.close()\n",
    "    print(passed)\n",
    "    print(failed)\n",
    "    print(passed*100/(passed+failed))\n",
    "    print(correct_samasa)\n",
    "    print(total_samasa)\n",
    "    print(correct_samasa*100/total_samasa)\n",
    "\n",
    "    return startlist\n",
    "\n",
    "test_model(dtest, model, char2idx, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb549473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and test files\n",
    "model.save('stage1_bilstm.h5')\n",
    "fh = open('stage1_char2idx.txt', 'w')\n",
    "data = str(char2idx)\n",
    "fh.write(data)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee2de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"dtest.csv\", \"w\")\n",
    "for data in dtest:\n",
    "    fp.write(data[0]+','+data[1]+'\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86533fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
